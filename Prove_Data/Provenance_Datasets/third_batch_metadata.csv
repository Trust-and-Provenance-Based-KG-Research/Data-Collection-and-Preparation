Approx Duration (ms),Video Description (Original),Video Title (Original),Video Publish Timestamp
2271000,"Multiclass semantic segmentation using U-Net with VGG, ResNet, and Inception as backbones.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 

https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com","210 - Multiclass U-Net using VGG, ResNet, and Inception as backbones",2021-03-24T07:00:02+00:00
1915000,"Multiclass semantic segmentation using Linknet and how does it compare against unet

Original paper on Unet: (2015)
    https://arxiv.org/pdf/1505.04597.pdf
    
Original paper on Linknet: (2017)
    https://arxiv.org/pdf/1707.03718.pdf
    
Can learn a bit more about backbone comparison here....
https://iopscience.iop.org/article/10.1088/1742-6596/1544/1/012196/pdf

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing



To annotate images and generate labels, you can use APEER (for free):
www.apeer.com

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models",211 - U-Net vs LinkNet for multiclass semantic segmentation,2021-03-31T07:00:10+00:00
1332000,"Classification of mnist hand sign language alphabets into 25 classes
(Z is not included as it includes a wave motion, not captured using a single image)
Dataset: https://www.kaggle.com/datamunge/sign-language-mnist

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist",212 - Classification of mnist sign language alphabets using deep learning,2021-04-07T07:00:00+00:00
1531000,"Classification of mnist hand sign language alphabets into 25 classes.
An ensemble of network results may provide improved accuracy compared to any single network. This video goes through the code that explains the ensemble process of the network predictions. 

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/datamunge/sign-language-mnist",213 - Ensemble of networks for improved accuracy in deep learning,2021-04-14T09:00:00+00:00
1773000,"Improving semantic segmentation (U-Net) performance via ensemble of multiple trained networks.
ResNet34 + Inception V3 + VGG16

If you get an error while loading Segmentation models library, please follow these steps to fix the issue: https://youtu.be/syJZxDtLujs

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing


To annotate images and generate labels, you can use APEER (for free):
www.apeer.com 

Segmentation models library documentation: 
https://github.com/qubvel/segmentation_models",214 - Improving semantic segmentation (U-Net) performance via ensemble of multiple trained networks,2021-04-21T07:00:04+00:00
1474000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",204 - U-Net for semantic segmentation of mitochondria,2021-02-25T08:00:08+00:00
1666000,"This video explains U-Net  segmentation of images followed by watershed
based separation of objects. Object properties will also be calculated.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset info: Electron microscopy (EM) dataset from
https://www.epfl.ch/labs/cvlab/data/data-em/

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",205 - U-Net plus watershed for instance segmentation,2021-03-02T08:00:09+00:00
3000000,"Can be applied to 3D volumes from FIB-SEM, CT, MRI, etc. (e.g., BRATS dataset). 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

If you get an error while loading Segmentation models library, please follow these steps to fix the issue: https://youtu.be/syJZxDtLujs

The dataset used in this video can be downloaded from the link below. This dataset can be used to train and test machine learning algorithms designed for multiclass semantic segmentation. Please read the Readme document for more information. 
https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view?usp=sharing

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com 

Segmentation models 3D library documentation: 
https://pypi.org/project/segmentation-models-3D/",215 - 3D U-Net for semantic segmentation,2021-04-28T07:00:03+00:00
86000,Just got bored of spending time at home so decided to get in the car and drive. I’m glad I found beautiful places that give some peace to mind!,Hidden gems around the Bay Area - Santa Cruz - Feb2021,2021-02-21T23:13:36+00:00
407000,"Link to sign up for Mito:
https://hubs.ly/H0H0Gsz0",Python Tips and Tricks - 1: Mito (trymito.io) for structured data,2021-02-22T20:04:04+00:00
408000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/02_Tips_Tricks_python_tips_and_tricks_2_google_image_download.py

This script downloads images from Google search (or Bing search). 

As with any download, please make sure you are not violating any copyright terms. I use this script to download images that help me practice deep learning based image classification. 

DO NOT use downloaded images to train a commercial product, this most certainly violates copyright terms. 

Do not pip install google_images_download

this gives an error that some images could not be downloadable. 
Google changed some things on their side...

The updated repo can be installed using the following command. 
pip install git+https://github.com/Joeclinton1/google-images-download.git

Please remember that this method has a limit of 100 images. 

OR

You can use bing.
Does not seem to have a limit on the number of images to download. 
pip install bing-image-downloader",Python tips and tricks - 2: Downloading images from online search,2021-02-23T19:02:13+00:00
133000,"In summary: DigitalSreeni is my personal channel and Apeer_Micro is my work channel. I appreciate if you subscribe to both.

Link to Apeer_Micro channel.
https://www.youtube.com/channel/UCVrG0AsRMb0pPcxzX75SusA",Clarification about my YouTube channels,2021-02-26T00:33:15+00:00
683000,"Image augmentation may hurt your model accuracy if you're not careful. Always test you augmentation operations first on a smaller dataset and then incrementally verify its accuracy before using it in your final model training. 

Link to the file from this video: https://github.com/bnsreenu/python_for_microscopists/blob/master/tips_tricks_3_data_augmentation.ipynb

Link to my GitHub account:
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 3: Be conservative with image augmentation,2021-03-05T23:07:51+00:00
543000,"imageJ (Fiji): https://imagej.net/Fiji

ZEN Lite: https://www.zeiss.com/microscopy/us/products/microscope-software/zen-lite.html",Python tips and tricks - 4: Best free software for image visualization and processing,2021-03-15T10:00:12+00:00
448000,"Link to my GitHub account:
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 5: Extracting patches from large images and masks for semantic segmentation,2021-03-23T10:00:02+00:00
2234000,"What to expect when you perform semantic segmentation using small datasets (less than 100 images) and U-net architecture? Does augmentation help and how about transfer learning?

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

To annotate images and generate labels, you can use APEER (for free):
www.apeer.com",216 - Semantic segmentation using a small dataset for training (& U-Net),2021-05-05T09:00:00+00:00
854000,"Step 1: Install Visual Studio community edition: I installed VS2019
https://visualstudio.microsoft.com/vs/community/

Step 2: Install your IDE if you haven't already done so. I installed Spyder 4.1.5 via Anaconda individual edition.
https://www.anaconda.com/products/individual

Step 3: Setup a separate environment for your IDE. I created an env. with python 3.7
I skipped 3.8 as it is relatively new and some of my libraries may not be tested or available. 
On Anaconda you can use Anaconda Navigaor to create a new environment. 
If you only see 3.8 as option on Navigator you can create new env from anaconda prompt: 
conda create -n py37gpu python=3.7 anaconda

For the following steps verify instructions from www.tensorflow.org/install/gpu

Step 4: Verify your GPU is actually supported for deep learning: 
https://developer.nvidia.com/cuda-gpus

Step 5: Figure out your GPU model: I have P5000 Quadro
and Update the GPU driver: For me it updated to Version 461.72 
https://www.nvidia.com/download/index.aspx?lang=en-us

Step 6: Download and install CUDA Toolkit. I installed Version 11.0
https://developer.nvidia.com/cuda-toolkit-archive

Step 7: Download cuDNN. I downloaded version 8.0.4
You need to sign up for Nvidia developer program (free)
Extract all folder contents from cudnn you just downloaded to 
C:\program files\Nvidia GPU computing toolkit\CUDA\v11.0

Step 8: Install tensorflow
Open spyder in the new environment that got named as: py37gpu (or whatever name you assigned)
- pip install tensorflow

Step 9: Verify the installation
Create a new python file and run these lines to test if GPU is recognized by tensorflow. 
import tensorflow as tf
tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)

The last few lines of my output shows...
physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:03:00.0, compute capability: 6.1)
physical GPU (device: 1, name: Quadro P5000, pci bus id: 0000:a1:00.0, compute capability: 6.1)

I have 2 GPUs. 

But on Windows you will not be able to efficiently use both GPUs for training. 

#On windows sysems you cannot install NCCL that is required for multi GPU
#So we need to follow Heirarchial copy method or reduce to single GPU.
 
strategy = tf.distribute.MirroredStrategy(devices=devices_names[:n_gpus],
                                           cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

#The following will raise an error on Windows about NCCL. 
#strategy = tf.distribute.MirroredStrategy(devices=devices_names[:n_gpus])",217 - 9 steps to installing TensorFlow GPU on Windows 10,2021-05-12T07:00:25+00:00
1088000,"Difference between UpSampling2D and Conv2DTranspose

These are the two common types of layers that can be used to increase the dimensions of arrays.

UpSampling2D is like the opposite of pooling where it repeats rows and columns of the input.

Conv2DTranspose performs up-sampling and convolution. 

Conv2DTranspose has been reported to result in Checkerboard artifacts but 
unfortunately not much information on the comparison of UpSampling2D vs Conv2DTranspose.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",218 - Difference between UpSampling2D and Conv2DTranspose used in U-Net and GAN,2021-05-19T07:00:20+00:00
2257000,"Understanding U-Net architecture and building it from scratch. 

This tutorial should clear any doubts you may have regarding the architecture of U-Net. It should also inform you on the process of building your own U-Net using functional blocks for encoder and decoder. 

Example use case: Segmentation of mitochondria using only 12 images and about 150 labeled objects.

Dataset: https://www.epfl.ch/labs/cvlab/data/data-em/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",219 - Understanding U-Net architecture and building it from scratch,2021-05-27T07:00:04+00:00
467000,"Note: Importing segmentation models library may give you generic_utils 
error on TF2.x

If you get an error about generic_utils...

change 
keras.utils.generic_utils.get_custom_objects().update(custom_objects) 
to 
keras.utils.get_custom_objects().update(custom_objects) 
in 
.../lib/python3.7/site-packages/efficientnet/__init__.py 

For Anaconda users:
Use this code to find out the location of site-packages directory 
under your current environment in anaconda. 

from distutils.sysconfig import get_python_lib
print(get_python_lib())


For Colab users:
You can click on the direct link provided on Colab for __init__.py and edit it.
Remember to restart the runtime after editing the file. 

Alternatively you can work with Tensorflow 1.x that doesn't throw
the generic_utils error. 
In google colab, add this as your first line.
%tensorflow_version 1.x
(Or just create a new environment in your local IDE to use TF1.x)

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 6: Fixing generic_utils error while importing segmentation models library,2021-03-30T07:00:07+00:00
2697000,"This video refers to semantic segmentation use case. 
Dataset: https://www.epfl.ch/labs/cvlab/data/data-em/

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/222_working_with_large_data_that_does_not_fit_memory_semantic_segm

Code for all videos: 
https://github.com/bnsreenu/python_for_microscopists

For semantic segmentation the folder structure needs to look like below
if you want to use ImageDatagenerator.

Data/
    train_images/
                train/
                    img1, img2, img3, ......
    
    train_masks/
                train/
                    msk1, msk, msk3, ......
                    
    val_images/
                val/
                    img1, img2, img3, ......                

    val_masks/
                val/
                    msk1, msk, msk3, ......
      
    test_images/
                test/
                    img1, img2, img3, ......    
                    
    test_masks/
                test/
                    msk1, msk, msk3, ......

Use split-folders library (pip install split-folders) to make this process of splitting
files easy.",222 - Working with large data that doesn't fit your system memory - Semantic Segmentation,2021-06-16T07:00:17+00:00
1141000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

# TTA - Should be called prediction time augmentation
#We can augment each input image, predict augmented images and average all predictions.",223 - Test time augmentation for semantic segmentation,2021-06-23T07:00:11+00:00
1211000,"IoU and Binary Cross-Entropy are good loss functions for binary semantic segmentation. but Focal loss may be better. 

Focal loss is good for multiclass classification where some classes are easy and other difficult to classify. ​It is just an extension of the cross-entropy loss.​ It down-weights easy classes and focuses training on hard to classify classes.​ In summary, focal loss turns the model’s attention towards the difficult to classify examples.​",220 - What is the best loss function for semantic segmentation?,2021-06-02T07:00:04+00:00
532000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

pip install split-folders

import splitfolders  # or import split_folders

input_folder = 'cell_images/'

# Split with a ratio.
# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.
#Train, val, test
splitfolders.ratio(input_folder, output=""cell_images2"", 
                   seed=42, ratio=(.7, .2, .1), 
                   group_prefix=None) # default values


# Split val/test with a fixed number of items e.g. 100 for each set.
# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.
# enable oversampling of imbalanced datasets, works only with fixed
splitfolders.fixed(input_folder, output=""cell_images2"", 
                   seed=42, fixed=(35, 20), 
                   oversample=False, group_prefix=None)","221 - Easy way to split data on your disk into train, test, and validation?",2021-06-09T07:00:06+00:00
548000,"This video briefly introduces you to the keras unet collection library that offers a few variants of the classic U-Net model. These variants include Attention U-Net, U-Net plus plus, and R2-U-Net. 

For more information about the library: https://github.com/yingkaisha/keras-unet-collection

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/
Images and masks are divided into patches of 256x256.",227 - Various U-Net models using keras unet collection library - for semantic image segmentation,2021-07-21T07:00:09+00:00
965000,"Residual Networks:
Residual networks were proposed to overcome the problems of deep CNNs (e.g., VGG). Stacking convolutional layers and making the model deeper hurts the generalization ability of the network. To address this problem, ResNet architecture was introduced which adds the idea of “skip connections”.

In traditional neural networks, each layer feeds into the next layer. In networks with residual blocks, each layer feeds into the next layer and directly into the layers about 2–3 hops away. Inputs can forward propagate faster through the residual connections (shortcuts) across layers.

Recurrent convolutional networks:
The recurrent network can use the feedback connection to store information over time. Recurrent networks use context information; as time steps increase, the network leverages more and more neighborhood information. Recurrent and CNNs can be combined for image-based applications. With recurrent convolution layers, the network can evolve over time though the input is static. Each unit is influenced by its neighboring units, includes the context information of an image.

U-net can be built using recurrent or residual or a combination block instead of the traditional double-convolutional block.",224 - Recurrent and Residual U-net,2021-06-30T07:00:06+00:00
896000,"What is attention and why is it needed for U-Net?

Attention in U-Net is a method to highlight only the relevant activations during training. It reduces the computational resources wasted on irrelevant activations and provides better generalization of the network. 

Two types of attention:

1. Hard attention
Highlight relevant regions by cropping.
One region of an image at a time; this implies it is non differentiable and needs reinforcement learning.  
Network can either pay attention or not, nothing in between.  
Backpropagation cannot be used. 

2. Soft attention
Weighting different parts of the image.
Relevant parts of image get large weights and less relevant parts get small weights. 
Can be trained with backpropagation. 
During training, the weights also get trained making the model pay more attention to relevant regions.  
In summary – it adds weights to pixels based on the relevance. 

Why is attention needed in U-Net?
U-net skip connection combines spatial information from the down-sampling path with the up-sampling path to retain good spatial information. But this process brings along the poor feature representation from the initial layers. Soft attention implemented at the skip connections will actively suppress activations at irrelevant regions.",225 - Attention U-net. What is attention and why is it needed for U-Net?,2021-07-07T07:00:04+00:00
1626000,"Is there a clear advantage of modified U-Net modules such as Attention U-Net and Residual U-Net over the standard U-Net? Watch the video to find out.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/
Images and masks are divided into patches of 256x256.",226 - U-Net vs Attention U-Net vs Attention Residual U-Net - should you care?,2021-07-14T07:00:13+00:00
543000,"Loading a keras model and continuing training​
When using custom loss function and metrics​.

No code to share with this video. 
Summary: Provide your custom optimizer or loss or metrics as custom objects during loading the model. 

my_model = load_model('your_trained_model.hdf5', 
                      custom_objects={'my_custom_loss': custom_loss,
                                      'custom_metric': my_custom_metric})",Python tips and tricks - 7:  Continuing keras model training when using custom loss and metrics,2021-05-08T08:00:08+00:00
1090000,"Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

RGB to HEX: (Hexadecimel -- base 16)
This number divided by sixteen (integer division; ignoring any remainder) gives 
the first hexadecimal digit (between 0 and F, where the letters A to F represent 
the numbers 10 to 15). The remainder gives the second hexadecimal digit. 
0-9 -- 0-9
10-15 -- A-F

Example: RGB -- R=201, G=, B=

R = 201/16 = 12 with remainder of 9. So hex code for R is C9 (remember C=12)

Calculating RGB from HEX: #3C1098
3C = 3*16 + 12 = 60
10 = 1*16 + 0 = 16
90 = 9*16 + 8 = 152

###############################
#CODE
#########################
#Convert Hex to RGB (numpy array)
Building = '#3C1098'.lstrip('#')
Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152

Land = '#8429F6'.lstrip('#')
Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246

Road = '#6EC1E4'.lstrip('#') 
Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228

Vegetation =  'FEDD3A'.lstrip('#') 
Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58

Water = 'E2A929'.lstrip('#') 
Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41

Unlabeled = '#9B9B9B'.lstrip('#') 
Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155

#########################
# Now replace RGB to integer values to be used as labels.
#Find pixels with combination of RGB for the above defined arrays...
#if matches then replace all values in that pixel with a specific integer
def rgb_to_2D_label(label):
    """"""
    Supply our labale masks as input in RGB format. 
    Replace pixels with specific RGB values ...
    """"""
    label_seg = np.zeros(label.shape,dtype=np.uint8)
    label_seg [np.all(label == Building,axis=-1)] = 0
    label_seg [np.all(label==Land,axis=-1)] = 1
    label_seg [np.all(label==Road,axis=-1)] = 2
    label_seg [np.all(label==Vegetation,axis=-1)] = 3
    label_seg [np.all(label==Water,axis=-1)] = 4
    label_seg [np.all(label==Unlabeled,axis=-1)] = 5
    
    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels
    
    return label_seg

labels = []
for i in range(mask_dataset.shape[0]):
    label = rgb_to_2D_label(mask_dataset[i])
    labels.append(label)    

labels = np.array(labels)   
labels = np.expand_dims(labels, axis=3)
 

print(""Unique labels in label dataset are: "", np.unique(labels))",Python tips and tricks - 8:  Working with RGB (and Hex) masks for semantic segmentation,2021-05-14T07:00:24+00:00
2517000,"This video demonstrates the process of pre-processing aerial imagery (satellite) data, including RGB labels to get them ready for U-net. The video also demonstrates the process of training a U-net and making predictions. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/228_semantic_segmentation_of_aerial_imagery_using_unet

My Github repo link: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are:

Building: #3C1098
Land (unpaved area): #8429F6
Road: #6EC1E4
Vegetation: #FEDD3A
Water: #E2A929
Unlabeled: #9B9B9B

Images come in many sizes: 797x644, 509x544, 682x658, 1099x846, 1126x1058, 859x838, 1817x2061,  2149x1479​

Need to preprocess so we can capture all images into numpy arrays. ​
Crop to a size divisible by 256 and extract patches.​

​Masks are RGB and information provided as HEX color code.​

Need to convert HEX to RGB values and then convert RGB labels to integer values and then to one hot encoded. ​

​Predicted (segmented) images need to converted back into original RGB colors. ​

​Predicted tiles need to be merged into a large image by minimizing blending artefacts (smooth blending). ​(Next video)",228 - Semantic segmentation of aerial (satellite) imagery using U-net,2021-07-28T07:00:20+00:00
1113000,"This video demonstrates the process of segmenting patches of images from a large image and blending patches back smoothly to minimize edge effects. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery

Original code for smooth blending is from here: https://github.com/Vooban/Smoothly-Blend-Image-Patches

The original code has been modified to fix a couple of bugs and chunks of code unnecessary for smooth tiling are removed.",229 - Smooth blending of patches for semantic segmentation of large images (using U-Net),2021-08-04T07:00:09+00:00
695000,"For example scaling inputs, performing preprocessing operations, converting masks ​to categorical​, etc. 

Code snippet from the video...

#Libraries
import segmentation_models as sm
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
from keras.utils import to_categorical

#Some scaling operation to be applied to images
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
from keras.utils import to_categorical

#Some preprocessing operation on images
BACKBONE = 'resnet34'
preprocess_input = sm.get_preprocessing(BACKBONE)

#Define a function to perform additional preprocessing after datagen.
#For example, scale images, convert masks to categorical, etc. 

def preprocess_data(img, mask, num_class):
    #Scale images
    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
    img = preprocess_input(img)  #Preprocess based on the pretrained backbone...
    #Convert mask to one-hot
    mask = to_categorical(mask, num_class)
      
    return (img,mask)

#Define the generator.
#We are not doing any rotation or zoom to make sure mask values are not interpolated.
#It is important to keep pixel values in mask as 0, 1, 2, 3, .....
from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
    
    img_data_gen_args = dict(horizontal_flip=True,
                      vertical_flip=True,
                      fill_mode='reflect')
    
    image_datagen = ImageDataGenerator(**img_data_gen_args)
    mask_datagen = ImageDataGenerator(**img_data_gen_args)
    
    image_generator = image_datagen.flow_from_directory(
        train_img_path,
        class_mode = None,
        batch_size = batch_size,
        seed = seed)
    
    mask_generator = mask_datagen.flow_from_directory(
        train_mask_path,
        class_mode = None,
        color_mode = 'grayscale',
        batch_size = batch_size,
        seed = seed)
    
    train_generator = zip(image_generator, mask_generator)
    
    for (img, mask) in train_generator:
        img, mask = preprocess_data(img, mask, num_class)
        yield (img, mask)

train_img_path = ""data/data_for_keras_aug/train_images/""
train_mask_path = ""data/data_for_keras_aug/train_masks/""
train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=4)

#Make sure the generator is working and that images and masks are indeed lined up. 
#Verify generator.... In python 3 next() is renamed as __next__()
x, y = train_img_gen.__next__()

for i in range(0,3):
    image = x[i]
    mask = np.argmax(y[i], axis=2)
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.subplot(1,2,2)
    plt.imshow(mask, cmap='gray')
    plt.show()",Python tips and tricks - 9: Performing additional tasks during data augmentation in keras,2021-05-24T07:00:07+00:00
2756000,"Semantic Segmentation of Landcover Dataset ​by loading images in batches from the drive​.

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/tree/master/230_landcover_dataset_segmentation

For all code: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://landcover.ai/
Labels:
0: Unlabeled background ​
1: Buildings​
2: Woodlands​
3: Water​

You can use any U-net but this code demonstrates the use of pre-trained encoder in the U-net - available as part of segmentation models library. 

To install the segmentation models library: pip install -U segmentation-models

If you are running into generic_utils error when loading segmentation models library watch this video to fix it: https://youtu.be/syJZxDtLujs.

Prepare the data first: 
1. Read large images and corresponding masks, divide them into smaller patches. And write the patches as images to the local drive.  

2. Save only images and masks where masks have some decent amount of labels other than 0. Using blank images with label=0 is a waste of time and may bias the model towards unlabeled pixels. 

3. Divide the sorted dataset from above into train and validation datasets. 

4. You have to manually move some folders and rename them appropriately if you want to use ImageDataGenerator from keras. 

After training, you can use the smooth blending process to segment large images.",230 - Semantic Segmentation of Landcover Dataset using U-Net,2021-08-11T07:00:00+00:00
910000,"Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information – 4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready 
Step 2: Define custom data generator
Step 3: Define the 3D U-net model
Step 4: Train and Predict

Step 1: Get the data ready
Download the dataset and unzip it. 
Segmented file name in Folder 355 has a weird name. Rename it to match others.
Install nibabel library to handle nii files (https://pypi.org/project/nibabel/)
Scale all volumes (using MinMaxScaler).
Combine the three non-native volumes (T2, T1CE and Flair) into a single multi-channel volume. 
Reassign pixels of value 4 to value 3 (as 3 is missing from original labels).
Crop volumes to remove useless blank regions around the actual volume of interest (Crop to 128x128x128).
Drop all volumes where the amount of annotated data is less that certain percentage. (To maximize training on real labeled volumes). 
Save all useful volumes to the local drive as numpy arrays (npy).
Split image and mask volumes into train and validation datasets. 

Step 2: Define custom data generator
Keras image data generator only works with jpg, png, and tif images. It will not recognize npy files. We need to define a custom generator to load our data from the disk. 

Step 3: Define the 3D U-net model
Extend the standard 2D U-Net into 3D OR 
copy the code from online OR 
use 3D segmentation models library 

Step 4: Train and Predict
Train by loading images in batches using our custom generator. 
Predict and plot data for visualization.",231 - Semantic Segmentation of BraTS2020 - Part 0 - Introduction (and plan),2021-08-18T07:00:01+00:00
1473000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information – 4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready (this video)
Step 2: Define custom data generator (next video)
Step 3: Define the 3D U-net model
Step 4: Train and Predict

Step 1: Get the data ready
Download the dataset and unzip it. 
Segmented file name in Folder 355 has a weird name. Rename it to match others.
Install nibabel library to handle nii files (https://pypi.org/project/nibabel/)
Scale all volumes (using MinMaxScaler).
Combine the three non-native volumes (T2, T1CE and Flair) into a single multi-channel volume. 
Reassign pixels of value 4 to value 3 (as 3 is missing from original labels).
Crop volumes to remove useless blank regions around the actual volume of interest (Crop to 128x128x128).
Drop all volumes where the amount of annotated data is less that certain percentage. (To maximize training on real labeled volumes). 
Save all useful volumes to the local drive as numpy arrays (npy).
Split image and mask volumes into train and validation datasets.",232 - Semantic Segmentation of BraTS2020 - Part 1 - Getting the data ready,2021-08-25T07:00:13+00:00
856000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information – 4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready (Previous video)
Step 2: Define custom data generator (This video)
Step 3: Define the 3D U-net model
Step 4: Train and Predict",233 - Semantic Segmentation of BraTS2020 - Part 2 - Defining your custom data generator,2021-09-01T07:00:06+00:00
1841000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.kaggle.com/awsaf49/brats20-dataset-training-validation

Dataset information:
Multimodal scans available as NIfTI files (.nii.gz) 

Four 'channels' of information – 4 different volumes of the same region
Native (T1) 
Post-contrast T1-weighted (T1CE)
T2-weighted (T2)
T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes

All the imaging datasets have been segmented manually and were approved by experienced neuro-radiologists. 

Annotations (labels): 
Label 0: Unlabeled volume
Label 1: Necrotic and non-enhancing tumor core (NCR/NET)
Label 2: Peritumoral edema (ED)
Label 3: Missing (No pixels in all the volumes contain label 3)
Label 4: GD-enhancing tumor (ET)

Our approach:
Step 1: Get the data ready
Step 2: Define custom data generator (Previous video)
Step 3: Define the 3D U-net model  (This video)
Step 4: Train and Predict  (This video)",234 - Semantic Segmentation of BraTS2020 - Part 3 - Training and Prediction,2021-09-08T07:00:19+00:00
1700000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The video summarizes the concept of autoencoders and walks you though the code for using autoencoder to reconstruct a single image. It also walks you through the code for displaying feature responses of various layer in a deep learning model.",235 - Pre-training U-net using autoencoders - Part 1 - Autoencoders and visualizing features,2021-09-15T07:00:01+00:00
1951000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset from: https://www.epfl.ch/labs/cvlab/data/data-em/

The video walks you through the process of training an autoencoder model and using the encoder weights for U-net.",236 - Pre-training U-net using autoencoders - Part 2 - Generating encoder weights for U-net,2021-09-22T07:00:13+00:00
708000,"This video explains the process of loading images and masks in the right order (in python) for semantic segmentation . 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Python tips and tricks - 10: Loading images and masks in the right order for semantic segmentation,2021-06-03T09:00:10+00:00
1169000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

To install opencv, you need to first install a lot of dependencies on the RPi.

These are the ones I installed to get it working on my RPi 3B.

sudo apt-get update 
sudo apt-get upgrade (consider full upgrade if you haven't used your Pi in a while)
                      
sudo apt-get install build-essential cmake pkg-config
sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng-dev
sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
sudo apt-get install libxvidcore-dev libx264-dev
sudo apt-get install libfontconfig1-dev libcairo2-dev
sudo apt-get install libgdk-pixbuf2.0-dev libpango1.0-dev
sudo apt-get install libgtk2.0-dev libgtk-3-dev
sudo apt-get install libatlas-base-dev gfortran
sudo apt-get install libhdf5-dev libhdf5-serial-dev libhdf5-103
sudo apt-get install libqtgui4 libqtwebkit4 libqt4-test python3-pyqt5
sudo apt-get install python3-dev

Now you can install opencv....
pip install opencv-contrib-python

Now, you need to install tflite interpreter.

You do not need full tensorflow to just run the tflite interpreter.
The package tflite_runtime only contains the Interpreter class which is what we need.
It can be accessed by tflite_runtime.interpreter.Interpreter.
To install the tflite_runtime package, just download the Python wheel
that is suitable for the Python version running on your RPi.

Here is the download link for the wheel files based on the Python version:
https://github.com/google-coral/pycoral/releases/
for Python 3.5, download: tflite_runtime-2.5.0-cp35-cp35m-linux_armv7l.whl (This is what I used in my video)

for Python 3.7, download: tflite_runtime-2.5.0-cp37-cp37m-linux_armv7l.whl


Download face and eye models:
Go to these links, click on RAW and save as... otherwise you'd be saving html files of Github page. '
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml","243 - Real time detection of facial emotion, age, and gender using TensorFlow Lite on RaspberryPi",2021-11-10T08:00:31+00:00
1841000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

First train a DL model and save it as h5. The convert to tflite. 
Dataset from: https://lhncbc.nlm.nih.gov/publication/pub9932

Binary problem: Question is: Is the cell in the image infected/parasited? 
If yes, probability is close to 1. 
If no, the probablility is close to 0. (uninfected)

This is because we added label 1 to parasited images. 
In summary, probability result close to 1 reflects infected (parasited) image 
and close to 0 reflects uninfected image",237 - What is Tensorflow Lite and how to convert keras model to tflite?,2021-09-29T07:00:05+00:00
1154000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Face and eye detection using opencv (Haar Cascade classificaion)

Download face and eye models:
Go to these links, click on RAW and save as... otherwise you'd be saving html files of Github page. '
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml
    https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml",238 - Real time face detection using opencv (and video feed from a webcam),2021-10-06T07:00:03+00:00
754000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Train a deep learning model on facial emotion detection
Dataset from: https://www.kaggle.com/msambare/fer2013

This trained model will be later used towards real time emotion detection on Windows and raspberry Pi.",239 - Deep Learning training for facial emotion detection,2021-10-13T07:00:11+00:00
637000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Train deep learning models to predict age and gender.
Dataset from here: https://susanqq.github.io/UTKFace/

This trained model will be later used towards real time emotion detection on Windows and raspberry Pi.",240 - Deep Learning training for age and gender detection,2021-10-20T07:00:13+00:00
709000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Live prediction of emotion, age, and gender using pre-trained models. 
Uses haar Cascades classifier to detect face. Then, uses pre-trained models for emotion, gender, and age to predict them from live video feed.","241 - Real time detection of facial emotion, age, and gender (using video feed from a webcam)",2021-10-27T07:00:11+00:00
1068000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Live prediction of emotion, age, and gender using pre-trained models. 
Uses haar Cascades classifier to detect face. Then it uses pre-trained models for emotion, gender, and age to predict them from live video feed. 

Prediction is done using tflite models. Note that tflite with optimization takes too long on Windows, so not even try.
Try it on edge devices, including RPi (next video).","242 - Real time detection of facial emotion, age, and gender using TensorFlow Lite (on Windows10)",2021-11-03T07:00:13+00:00
394000,Amazon link to the book: https://www.amazon.com/Automated-Machine-Learning-AutoKeras-accessible/dp/1800567642/ref=sr_1_2?dchild=1&keywords=autokeras&qid=1624553154&sr=8-2,My review of the 'Automated Machine Learning with AutoKeras' book,2021-06-25T18:59:56+00:00
601000,Career tips on 5 things to check before applying for your first machine learning job.,5 things to check before applying for your first machine learning job,2021-07-19T07:00:04+00:00
393000,A couple of career tips for those wanting to become a machine learning engineer.,"You want to be a machine learning engineer, now what?",2021-07-12T07:00:03+00:00
1104000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Embedding layer...
Maps each value in the input array to a vector of a defined size.​
The weights in this layer are learned during the training process.​
Initialization is performed (just like other keras layers).​

One-hot encoding is inefficient as most indices are zero. (e.g., Text with 1000 words means most of the elements are 0) ​
Integer encoding does not reflect the relationship between words. ​
Embedding allows for the representation of similar words with similar encoding.​
Values are learned (trainable).​",244 - What are embedding layers in keras?,2021-11-17T08:00:13+00:00
718000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 14 - EasyOCR for text detection in images (using python),2021-08-02T07:00:28+00:00
1225000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",245 - Advantages of keras functional API in defining complex models,2021-11-24T08:00:27+00:00
1068000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",246 - Training a keras model by enumerating epochs and batches,2021-12-01T08:00:09+00:00
2391000,"Conditional Generative Adversarial Network  cGAN

A GAN model generates a random image from the domain. 
The relationship between points in the latent space and the generated images is hard to map.
A GAN can be trained so that both the generator and the discriminator models are conditioned on the class label (or other modalities). 
As a result, the trained generator model can be used to generate images of a given type using the class label (or another condition).
GAN can be conditioned using other image modalities (image to image translation).
The conditioning is performed by feeding the class label into both the discriminator and generator as an additional input layer.

A few applications:
Image-to-Image Translation: Pix2Pix GAN
CycleGAN: Transform images from one set into images that could belong to another set. 
Super-resolution: Increase the resolution of images, adding detail where necessary to fill in blurry areas. 
Text-to-Image Synthesis: Take text as input and produce images as described by the text.",247 - Conditional GANs and their applications,2021-12-08T08:00:08+00:00
684000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Python tips and tricks - 13
How to plot keras models using plot_model on Windows10

We use the plot_model library:​
from tensorflow.keras.utils import plot_model​

Plot_model requires Pydot and graphviz libraries.​

To install Graphviz: ​
Download and install the latest version exe​
https://gitlab.com/graphviz/graphviz/-/releases ​

To check the installation,​
go to the command prompt and enter: dot -V​

Open Anaconda prompt for the ​desired environment ​

pip install pydot​
pip install graphviz​",Tips Tricks 13 - How to visualize keras models on windows10,2021-07-26T07:00:17+00:00
328000,"Week of 12-18 July 2021

Links to the content referenced in the video:
https://iterative-refinement.github.io/
https://arxiv.org/pdf/2104.07636.pdf
https://www.nature.com/articles/s41598-021-93889-z.pdf
https://www.nature.com/articles/s41467-021-23952-w.pdf
https://cdn.openai.com/papers/jukebox.pdf",What I am reading this week about Machine Learning and AI - 16 July 2021,2021-07-16T07:00:12+00:00
1796000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Conditional Generative Adversarial Network  cGAN

A GAN model generates a random image from the domain. 
The relationship between points in the latent space and the generated images is hard to map.
A GAN can be trained so that both the generator and the discriminator models are conditioned on the class label (or other modalities). 
As a result, the trained generator model can be used to generate images of a given type using the class label (or other condition).
GAN can be conditioned using other image modalities (image to image translation).
The conditioning is performed by feeding the class label into both the discriminator and generator as additional input layer.

A few applications:
Image-to-Image Translation: Pix2Pix GAN
CycleGAN: Transform images from one set into images that could belong to another set. 
Super-resolution: Increase the resolution of images, adding detail where necessary to fill in blurry areas. 
Text-to-Image Synthesis: Take text as input and produce images as described by the text.",249 - keras implementation of Conditional GAN (cifar10 data set),2021-12-22T08:00:07+00:00
1869000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",248 - keras implementation of GAN to generate cifar10 images,2021-12-15T08:00:08+00:00
1973000,"A review of the original publication. https://arxiv.org/abs/1611.07004

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The discriminator in the Pix2Pix GAN is implemented as a PatchGAN.
PatchGAN discriminator tries to classify if each N×N patch in an image is real or fake. (as opposed to classifying an entire image)
This discriminator is run convolutionally across the image, averaging all responses to provide the final output.
The receptive field in a PatchGAN represents the relationship between one output activation to an area on the input image. 
A 70×70 PatchGAN will classify 70×70 patches of the input image as real or fake.",250 - Image to image translation using Pix2Pix GAN,2021-12-29T08:00:08+00:00
1368000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Data from: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz
Also find other datasets here: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/

Original pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf
Github for the original paper: https://phillipi.github.io/pix2pix/",251 - Satellite image to maps translation using  pix2pix GAN,2022-01-05T08:00:05+00:00
1306000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset link: https://drive.google.com/file/d/1HWtBaSa-LTyAMgf2uaz1T9o1sTWDBajU/view
(Please read the Readme document in the dataset folder for more information. )",252 - Generating realistic looking scientific images using pix2pix GAN,2022-01-12T08:00:30+00:00
1552000,"(No code in this tutorial, please watch the next tutorial for keras implementation)
Original paper: https://arxiv.org/abs/1703.10593

The model uses instance normalization layer:
Normalize the activations of the previous layer at each step,
i.e. applies a transformation that maintains the mean activation
close to 0 and the activation standard deviation close to 1.
Standardizes values on each output feature map rather than across features in a batch. ​

Download instance normalization code from here: https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/normalization/instancenormalization.py
Or install keras_contrib using guidelines here: https://github.com/keras-team/keras-contrib",253 - Unpaired image to image translation​ using cycleGAN - An introduction,2022-01-19T08:00:08+00:00
2307000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Original CycleGAN paper: https://arxiv.org/abs/1703.10593

Dataset from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/",254 - Unpaired image to image translation​ using cycleGAN in keras,2022-01-26T08:00:00+00:00
1763000,"Single Image Super-Resolution Using SRGAN

Understanding the concept by walking through the original publication.​
Original paper: https://arxiv.org/pdf/1609.04802.pdf",255 - Single image super resolution​ using SRGAN,2022-02-02T08:00:14+00:00
1366000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Original paper: https://arxiv.org/pdf/1609.04802.pdf

Dataset from:
    http://press.liacs.nl/mirflickr/mirdownload.html
    
All images resized to 128x128 to represent HR and 32x32 to represent LR.",256 - Super resolution GAN (SRGAN) in keras,2022-02-09T08:00:00+00:00
388000,"Week of 12-18 July 2021

https://www.biorxiv.org/content/10.1101/2021.07.19.452964v1.full.pdf
https://arxiv.org/pdf/2103.10697.pdf
https://bair.berkeley.edu/blog/2021/07/22/spml/
https://arxiv.org/pdf/2105.00957.pdf
https://arxiv.org/pdf/2101.06307.pdf
https://jisrc.szabist.edu.pk/JISRC/Papers/JISR-021-10.pdf
https://www.journals.resaim.com/ijresm/article/view/1018/983
https://irjmets.com/rootaccess/forms/uploads/IRJMETS149474.pdf",What I am reading this week about Machine Learning and AI - 23 July 2021,2021-07-23T17:00:22+00:00
2339000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

UTKFace dataset (Used in this video): https://susanqq.github.io/UTKFace/

Haarcascade models, if interested in detecting faces and extracting them into new images.
https://github.com/opencv/opencv/tree/master/data/haarcascades
Celeb Dataset (Not used in the video): 
https://www.kaggle.com/jessicali9530/celeba-dataset

Description: 
Latent space is hard to interpret unless conditioned using many classes.​
But, the latent space can be exploited using generated images.​
Here is how...

- Generate 10s of images using random latent vectors.​
- Identify many images within each category of interest (e.g., smiling man, neutral man, etc. )​
- Average the latent vectors for each category to get the mean representation in the latent space (for that category).​
- Use these mean latent vectors to generate images with features of interest. ​
In summary, you can find the latent vectors for Smiling Man, neutral face man, and a baby with a neutral face and then generate a smiling babyface by:

    Smiling Man + Neutral Man - Neutral baby = Smiling Baby",257 - Exploring GAN latent space to generate images with desired features​,2022-02-16T08:00:13+00:00
1109000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Cross-entropy is a measure of the difference between two probability distributions.​",Tips Tricks 15 - Understanding Binary Cross-Entropy loss,2021-08-09T07:00:10+00:00
52000,"Coyote Hills Regional Park - San Francisco East Bay

https://www.ebparks.org/parks/coyote_hills/",🚴‍♀️ around the Coyote Hills Regional Park - San Francisco East Bay,2021-07-31T21:10:14+00:00
1008000,"Rough calculation to estimate the required memory (esp. GPU) to train a deep learning model. 

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 16 - How much memory to train a DL model on large images,2021-08-16T07:00:30+00:00
2465000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset:
https://www.kaggle.com/uciml/pima-indians-diabetes-database

A decorator in python allows us to add new functionality to an existing 
object (function or class) by not requiring us to modify the object's structure. 

Decorators allow us to wrap another function to extend the behavior of the wrapped function, without permanently modifying it. They are typically called before defining another function that we'd like to decorate.

Functions are first-class objects in python. This means they support 
the following operations. 

- Stored in a variable. 
- Passed as an argument to another function. 
- Defined inside another function.
- Returned from another function. 
- Store in data structures such as lists.

Decorators leverage this behavior of functions.",Tips Tricks 17 - All you need to know about decorators in python,2021-08-23T07:00:16+00:00
372000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Dataset: https://www.kaggle.com/jessicali9530/celeba-dataset

Haarcascade models...
https://github.com/opencv/opencv/tree/master/data/haarcascades",Tips Tricks 18 - Extracting faces from images for deep learning training,2021-08-30T07:00:01+00:00
450000,"References from the video:
Sketch Your Own GAN: 
https://arxiv.org/pdf/2108.02774.pdf

LARGE: Latent-Based Regression through GAN Semantics: 
https://arxiv.org/pdf/2107.11186.pdf

PathML: A unified framework for whole-slide image analysis with deep learning:
https://www.medrxiv.org/content/10.1101/2021.07.07.21260138v1.full.pdf

SofGAN: A GAN Face Generator That Offers Greater Control
https://www.unite.ai/sofgan-a-gan-face-generator-that-offers-greater-control/

BOOK: Making It Personal: How To Profit From Personalization Without Invading Privacy
Author: Bruce Kasanoff 
Please search your local book store for this book.",What I am reading this week about Machine Learning and AI - 13 August 2021,2021-08-13T07:00:12+00:00
2132000,"Confused about paying for cloud-based systems versus purchasing your own workstation? Hopefully, this video can shed some light.",Tips Tricks 19 - colab vs colab pro vs purchasing your own system,2021-09-06T07:00:09+00:00
2058000,"Semi-supervised learning with generative adversarial networks. 

Semi-supervised refers to the training process where the model gets trained only on a few labeled images but the data set contains a lot more unlabeled images. This can be useful in situations where you have a humongous data set but only partially labeled. 

In regular GAN the discriminator is trained in an unsupervised manner, where it predicts whether the image is real or fake (binary classification). In SGAN, in addition to unsupervised, the discriminator gets trained in a supervised manner on class labels for real images (multiclass classification). 

In essence, the unsupervised mode trains the discriminator to learn features and the supervised mode trains on corresponding classes (labels). The GAN
can be trained using only a handful of labeled examples. 

In a standard GAN our focus is on training a generator that we want to use to generate fake images. In SGAN, our goal is to train the discriminator to be an excellent classifier using only a few labeled images. We can still use the generator to generate fake images but our focus is on the discriminator. 

Why do we want to follow this path is CNNs can easily classify images?
Apparently, this approach achieves better accuracy for limited labeled data compared to CNNs. 
(https://arxiv.org/abs/1606.01583)

Another useful resource: https://arxiv.org/pdf/1606.03498.pdf​",258 - Semi-supervised learning with GANs,2022-02-23T08:00:08+00:00
1753000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Semi-supervised learning with generative adversarial networks. 

Semi-supervised refers to the training process where the model gets trained only on a few labeled images but the data set contains a lot more unlabeled images. This can be useful in situations where you have a humongous data set but only partially labeled. 

In regular GAN the discriminator is trained in an unsupervised manner, where it predicts whether the image is real or fake (binary classification). In SGAN, in addition to unsupervised, the discriminator gets trained in a supervised manner on class labels for real images (multiclass classification). 

In essence, the unsupervised mode trains the discriminator to learn features and the supervised mode trains on corresponding classes (labels). The GAN
can be trained using only a handful of labeled examples. 

In a standard GAN our focus is on training a generator that we want to use to generate fake images. In SGAN, our goal is to train the discriminator to be an excellent classifier using only a few labeled images. We can still use the generator to generate fake images but our focus is on the discriminator. 

Why do we want to follow this path is CNNs can easily classify images?
Apparently, this approach achieves better accuracy for limited labeled data compared to CNNs. 
(https://arxiv.org/abs/1606.01583)

Another useful resource: https://arxiv.org/pdf/1606.03498.pdf​",259 - Semi-supervised learning with GANs - in keras,2022-03-02T08:00:25+00:00
2009000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Detecting anomaly images using AutoEncoders. 
(Sorting an entire image as either normal or anomaly)

Here, we use both the reconstruction error and also the kernel density estimation
based on the vectors in the latent space. We will consider the bottleneck layer output
from our autoencoder as the latent space. 

This code uses the malarial data set but it can be easily applied to 
any application. 

Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html",260 - Identifying anomaly images using convolutional autoencoders,2022-03-09T08:00:16+00:00
1585000,"Anomaly localization in images using the global average pooling layer.

Binary classification - Good vs. bad images (Uninfected vs parasiized)
This code uses the malarial data set but it can be easily applied to 
any application. 

Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",262 - Localizing anomalies in images,2022-03-23T07:00:13+00:00
911000,"What is the Global Average Pooling (GAP layer) and how it can be used to summrize features in an image?

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",261 - What is global average pooling in deep learning?,2022-03-16T07:00:16+00:00
963000,"Object localization in an image by leveraging the global average pool layer. 

Imagenet classes can be obtained from here:
    https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a
    OR
    https://github.com/Waikato/wekaDeeplearning4j/blob/master/docs/user-guide/class-maps/IMAGENET.md

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists",263 - Object localization in images​ using GAP layer,2022-03-30T07:00:08+00:00
1846000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Outlier detection using alibi-detect

Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. The outlier detection methods should allow the user to identify global, contextual, and collective outliers.

pip install alibi-detect

https://github.com/SeldonIO/alibi-detect
Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/

We will be using VAE based outlier detection. Based on this paper:
https://arxiv.org/pdf/1312.6114.pdf
    
The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch of unlabeled, but normal (inlier) data. Unsupervised training is desirable since labeled data is often scarce. The VAE detector tries to reconstruct the input it receives. If the input data cannot be reconstructed well, the reconstruction error is high and the data can be flagged as an outlier. The reconstruction error is either measured as the mean squared error (MSE) between the input and the reconstructed instance or as the probability that both the input and the reconstructed instance are generated by the same process.

Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf
Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad",264 - Image outlier detection using alibi-detect,2022-04-06T07:00:11+00:00
2852000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

The input to the Vgg 16 model is 224x224x3 pixels images. 
The Kernel size is 3x3 and the pool size is 2x2 for all the layers.

If our image size is different, can we still use transfer learning?
The answer is YES.

Input image size does not matter as the weights are associated with the filter kernel size. This does not change based on the input image size, for convolutional layers. 

The number of channels does matter, as it affects the number of weights for the first convolutional layer. We can still use transfer learning by copying weights for the first channels from the original model and then filling the additional channel weights with the mean of existing weights along the channels.",Tips Tricks 20 - Understanding transfer learning for different size and channel inputs,2021-09-13T07:00:04+00:00
1860000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

What is a better approach when working with small training data for semantic segmentation? Is it deep learning such as U-net or is it feature extraction followed by machine learning classification (e.g., Random Forest, LGBM, XGBoost, SVM, etc.)?",265 - Feature engineering or deep learning (for semantic segmentation),2022-04-13T07:00:08+00:00
1724000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Demo using H&E normalization and H / E signal separation.
Here, we use openslide to read a whole slide image. 
We will then extract a lower reolution version of the image to normalize it and then to extract H and E signals separately. 

We will also perform the exact operation on the entire whole slide image by extracting tilee, processing them, and saving processed images separately. 

Please note that this code will not cover putting tiles back into a whole slide image (image pyramid). You can explore pyvips or similar package to put together tiles into an image pyramid. 

For an introduction to openslide, please watch video 266: 
    
For details about H&E normalization, please watch my video 122: https://youtu.be/yUrwEYgZUsA
    
Useful references:
A method for normalizing histology slides for quantitative analysis. M. Macenko et al., ISBI 2009
http://wwwx.cs.unc.edu/~mn/sites/default/files/macenko2009.pdf

Efficient nucleus detector in histopathology images. J.P. Vink et al., J Microscopy, 2013

Other useful references:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5226799/
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169875",267 - Processing whole slide images (as tiles),2022-04-27T07:00:12+00:00
2017000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

OpenSlide can read virtual slides in several formats:
Aperio (.svs, .tif)
Hamamatsu (.ndpi, .vms, .vmu)
Leica (.scn)
MIRAX (.mrxs)
Philips (.tiff)
Sakura (.svslide)
Trestle (.tif)
Ventana (.bif, .tif)
Generic tiled TIFF (.tif)

OpenSlide allows reading a small amount of image data at the resolution 
closest to a desired zoom level.

pip install openslide-python

then download the latest windows binaries
https://openslide.org/download/

Extract the contents to a place that you can locate later.

If you are getting the error: [WinError 126] The specified module could not be found

Open the lowlevel.py file located in:
    lib\site-packages\openslide
    
Add this at the top, after from __future__ import division, in the lowlevel.py
os.environ['PATH'] = ""path+to+binary"" + "";"" + os.environ['PATH']
path+to+binary is the path to your windows binaries that you just downloaded.

In my case, it looks like this.

import os
os.environ['PATH'] = ""C:/Users/Admin/anaconda3/envs/py37/lib/site-packages/openslide/openslide-win64-20171122/bin"" + "";"" + os.environ['PATH']

A few useful commands to locate the sitepackages directory

import sys
for p in sys.path:
    print(p)",266 - Openslide library for whole slide images,2022-04-20T07:00:16+00:00
10000,,"Web-deployed deep learning model, on Heroku.",2021-09-15T23:37:16+00:00
1750000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video explains the process of using Flask to deploy your scikit-learn (or other) trained model into a web application.",268 - How to deploy your trained machine learning model into a local web application?,2022-05-04T07:00:07+00:00
1541000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained machine learning model as a browser-based app on Heroku (www.heroku.com)",269 - How to deploy your trained machine learning model as a web app on Heroku?,2022-05-11T07:00:27+00:00
2428000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained deep learning model as a browser-based app on Heroku. It uses HAM10000 trained model to predict skin cancer type for the image supplied by the user. 

Some useful commands:
Please install Heroku CLI on your system.

To log in to your Heroku account from CLI:
heroku login -i​

To scale dynos for your app:
heroku ps:scale web=1 --app app_name",270 - How to deploy your trained machine learning model as a web app on Heroku (No Docker),2022-05-18T07:00:21+00:00
1954000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video demonstrates the process of deploying your trained deep learning model as a browser-based app on Heroku by deploying a Docker container. It uses HAM10000 trained model to predict skin cancer type for the image supplied by the user. This application is exactly the same as the one from video 270 except here we use Docker. 


Some useful commands:
Please install Heroku CLI on your system.

To log in to your Heroku account from CLI:
heroku login -i​

To scale dynos for your app:
heroku ps:scale web=1 --app app_name

Docker:
Please install Docker on your system (e.g. Docker for Windows)
Our app name from this video is used as an example for the following commands. 

To build an image using the Docker file:
docker image build -t skincancer-app .

List Docker images:
docker image ls

To run Docker on a specific port:
docker run -p 5000:5000 -d skincancer-app

Open the URL to check your app.
http://localhost:5000/

To list docker containers:
docker container ls

To stop a container:
docker container stop container id

To clear all containers and cleanup:
docker system prune

To remove a Docker image: (check the image ID via: docker image ls)
docker image rm image_id

______________________

Heroku Docker deployment process from command prompt. 

heroku container:login

heroku create name-for-your-app

heroku container:push web --app name-for-your-app

heroku container:release web --app name-for-your-app

heroku ps:scale web=1 --app app_name",271 - How to deploy your trained machine learning model as a web app on Heroku (with docker),2022-05-25T07:00:09+00:00
1368000,"This video explains the process of exploring keras model saved as hdf5 (or .h5).

To download the HDF Viewer: https://www.hdfgroup.org/downloads/hdfview/",Tips Tricks 21 - Understanding the keras-trained model saved as hdf5 (or h5),2021-10-04T07:00:17+00:00
831000,"Link to the book on Amazon: https://www.amazon.com/Deep-Learning-fastai-Cookbook-easy/dp/1800208103/ref=sr_1_4?dchild=1&keywords=deep+learning+with+fastai&qid=1634237733&sr=8-4

Fastai API paper (pdf): https://arxiv.org/pdf/2002.04688.pdf",Book Review - Deep Learning with fastai Cookbook,2021-10-16T13:30:27+00:00
1728000,"Lung cancer subclassification using fastai​ 

Fastai API info: https://arxiv.org/pdf/2002.04688.pdf

Direct link to the colab notebook:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_22_fastai_lung_cancer_classification.ipynb

Data set information:
https://github.com/tampapath/lung_colon_image_set/
https://arxiv.org/ftp/arxiv/papers/1912/1912.12142.pdf
LC25000 Lung and colon histopathological image dataset from: https://academictorrents.com/details/7a638ed187a6180fd6e464b3666a6ea0499af4af

The dataset contains color 25,000 images with 5 classes of 5,000 images each. All images are 768 x 768 pixels in size and are in jpeg file format. Our dataset can be downloaded as a 1.85 GB zip file LC25000.zip. After unzipping, the main folder lung_colon_image_set contains two subfolders: colon_image_sets and lung_image_sets.

The subfolder lung_image_sets contains three secondary subfolders: lung_aca subfolder with 5000 images of lung adenocarcinomas, lung_scc subfolder with 5000 images of lung squamous cell carcinomas, and lung_n subfolder with 5000 images of benign lung tissues.",Lung cancer subclassification using fastai​ (Tips Tricks 22),2021-10-23T07:00:06+00:00
1853000,"Code from this video is available here:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_23_COVID_vaccine_analysis.ipynb

COVID vaccination data can be downloaded from here: 
https://www.kaggle.com/gpreda/covid-world-vaccination-progress",COVID Vaccine analysis using pandas in python (Tips Tricks 23),2021-11-06T07:00:09+00:00
1324000,"pyviz documentation: https://pyvis.readthedocs.io/en/latest/

Code used in this video is available here:
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_24_quick_intro_to_pyviz.ipynb",Tips Tricks 24 - Interactive network visualization using pyviz,2021-11-20T08:00:08+00:00
781000,"Locating objects in large images using template matching
(opencv in python)

Code from this video is available here: https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_25_locating_objects_in_large_images_via_template_matching.py",Tips Tricks 25 - Locating objects in large images using template matching,2021-11-22T22:42:46+00:00
751000,"7 best machine learning books in 2022

Basics:
The hundred-page machine learning book
Deep learning with python (François Chollet)
Automated machine learning with AutoKeras
Deep learning for computer vision with python

Advanced:
Deep Learning (Ian Goodfellow)
Pattern recognition and machine learning
Artificial intelligence, A modern approach - Fourth Edition",7 best machine learning books in 2022,2021-12-24T08:00:12+00:00
778000,"Just normalize your 16 bit image to its respective maximum pixel value and then convert to uint8 using numpy or opencv or scikit-image

Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_Tricks_26_proper-way_to_convert_16bit_to_8bit_image.py

For my other code:
https://github.com/bnsreenu/python_for_microscopists",Tips Tricks 26 - How to properly convert 16 bit to 8 bit images in python,2022-01-15T08:00:02+00:00
293000,Link to the book on Amazon: https://www.amazon.com/Machine-Learning-Biotechnology-Life-Sciences/dp/1801811911,Book Review - Machine Learning in Biotechnology and Life Sciences,2022-03-05T08:00:03+00:00
756000,"Do not waste your time doing manual things that can be automated. This video walks you through the simple python code to extract required information from your Outlook inbox. You can find the code to this video here:
https://github.com/bnsreenu/python_for_microscopists/tree/master/AMT01_extracting_information_from_outlook_emails

For other code, checkout my GitHub repo: https://github.com/bnsreenu/python_for_microscopists",AMT1 - Extracting required information from your Outlook inbox,2022-02-19T00:22:39+00:00
962000,"Do not waste your time doing manual things that can be automated. This video walks you through the simple python code to extract required information from your Gmail inbox. A great way pf compiling text for your natural language processing and other machine learning projects. 

You can find the code to this video and other videos here:
https://github.com/bnsreenu/python_for_microscopists",AMT2 - Extracting Emails from your Gmail Inbox using python,2022-02-26T08:00:05+00:00
1628000,"The code snippet for this video can be downloaded from:  
https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_27_labeling_images_for_sem_segm_using_label_studio.py

For other code available on my GitHub:
https://github.com/bnsreenu/python_for_microscopists

For labeling your images using Label Studio:
https://labelstud.io/

Let us work in Anaconda command prompt. (You can use other command prompts)
Check environments: 
conda env list

Create a new environment to install Label Studio:
conda create --name give_some_name pip
(Need to specify pip as a dependency, otherwise it will not be available)

(To specify python version for your env..)
conda create -n give_some_name python=3.7

Now activate the env.
conda activate give_some_name

# Install the Label Studio package
pip install -U label-studio

# Launch it!
label-studio

Open your browser and go to the URL displayed on your screen, typically
http://0.0.0.0:8080/",Labeling images for semantic segmentation using Label Studio,2022-03-12T08:00:11+00:00
645000,"Part of the Tips and tricks series - Number: 28 

Download QuPath from:
https://qupath.github.io/

Download the groovy script from here:
https://raw.githubusercontent.com/stardist/stardist/master/extras/qupath_export_annotations.groovy",Labeling images using QuPath for semantic segmentation,2022-03-19T07:00:01+00:00
946000,"Part of the Tips and tricks series - Number: 29

Download FiJi from here:
https://imagej.net/software/fiji/

Install the LabKit plugin:
https://imagej.net/plugins/labkit/",Labeling images using LabKit for semantic segmentation,2022-03-26T07:00:05+00:00
1574000,"Part of the Tips and tricks series - Number: 30 
Random gets used quite often in python for data analysis and machine learning. This is an explainer video on the topic of 'random' and random seeds. Also, learn about the birthday paradox.

Python random uses Mersenne Twister algorithm:
https://en.wikipedia.org/wiki/Mersenne_Twister

Numpy random uses the Permuted congruential generator algorithm:
https://en.wikipedia.org/wiki/Permuted_congruential_generator

More about the Birthday paradox/problem:
https://en.wikipedia.org/wiki/Birthday_problem#Understanding_the_problem",Random is not so random - understanding random in python,2022-04-02T07:00:01+00:00
542000,"Tips tricks 31 - generating borders around objects

Code on my GitHub: https://github.com/bnsreenu/python_for_microscopists

Create border pixels from binary masks. We can include these border pixels as another class to train a multiclass semantic segmenter. 
What is the advantage? We can use border pixels to perform watershed and achieve 'instance' segmentation.",Generating borders around objects for use in semantic segmentation,2022-04-09T07:00:02+00:00
2028000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

This video goes through the process of adding borders to binary objects, then using them as masks to train a multiclass U-net model, and finally segmenting images using the trained model followed by watershed separation.",272 - Instance segmentation via semantic segmentation by using border class,2022-06-01T07:00:05+00:00
1213000,"Code generated in the video can be downloaded from here: 
https://github.com/bnsreenu/python_for_microscopists

Object segmentation and analysis using voronoi otsu labeling in the pyclesperanto library in python

We will be using a multichannel CZI (Zeiss) input image for this exercise.
This requires czi file library. 

pip install czifile

For standard images (e.g., jpg, tif, etc.) use skimage, cv2, or tifffile to read
input images. 

# For installation instructions of the pyclesperanto package, 
please refer to the following link
# https://github.com/clEsperanto/pyclesperanto_prototype",275 - Object segmentation and analysis using voronoi otsu labeling,2022-06-22T07:00:12+00:00
